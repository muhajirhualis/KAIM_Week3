
# KAIM Week 3 Challenge: Insurance Risk Analytics  
*AlphaCare Insurance Solutions (ACIS) â€” Optimizing Premium Strategy via EDA & Reproducible Pipelines*

> *"Discover low-risk segments and build auditable analytics for dynamic, risk-based pricing in South African auto insurance."*

---

##  Business Objective
ACIS aims to identify **low-risk customer segments** (e.g., specific provinces, vehicle models, demographics) where premiums can be **strategically reduced** to attract new clients â€” without compromising portfolio profitability.

This project delivers:
- Foundational EDA insights on profitability, risk drivers, and data quality  
- Reproducible, auditable data pipeline using **Data Version Control (DVC)**  
- Statistical groundwork for hypothesis testing (Task 3) and modeling (Task 4)

---

## Tech Stack
| Category | Tools |
|--------|-------|
| **Language** | Python 3.10+ |
| **Core Libraries** | `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn` |
| **Data Engineering** | Modular OOP pipelines (`DataLoader`, `DataPreprocessor`, `EDAPipeline`) |
| **Reproducibility** | `DVC` (Data Version Control), `Git` |
| **Environment** | `venv`/`conda`, `requirements.txt` |

---

## Project Structure
```
KAIM_Week3/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ MachineLearningRating_v3.txt      # â† Raw data (DVC-tracked)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py        # Robust data ingestion
â”‚   â”œâ”€â”€ eda_preprocess.py     # Type correction, missing value imputation, feature engineering
â”‚   â””â”€â”€ eda_pipeline.py       # EDA visualizations & statistics
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda_stats.ipynb      # Full EDA workflow & insights
â”œâ”€â”€ .dvc/                     # DVC config & cache metadata
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Task 1: Exploratory Data Analysis & Statistics

### Key Deliverables
- Comprehensive univariate & bivariate analysis
- Answers to 4 business-guiding questions (see below)
- 3+ insightful visualizations (including loss ratio segmentation & log-scaled financial distributions)
- Modular preprocessing pipeline for modeling readiness

### Core Insights
| Guiding Question | Finding | Business Implication |
|------------------|---------|----------------------|
| **1. What is the overall Loss Ratio?** | **LR = 1.05** (R64.9M claims / R61.9M premiums) | **Portfolio is unprofitable** â€” loses ~5c per Rand of premium. Requires urgent repricing or risk segmentation. |
| **2. How do financial variables distribute?** | `TotalClaims` & `TotalPremium` are **highly right-skewed** (median=0, max > R390k). 100% missing in `NumberOfVehiclesInFleet`. | Outliers dominate mean; use **log-transforms** (`np.log1p`) in modeling. Fleet indicator created for individual policies. |
| **3. Are there temporal trends?** | Claim **severity is rising** over 18 months (Mar 2014 â†’ Aug 2015), while frequency is stable. | Signals increasing repair costs â€” premiums must be dynamically adjusted for inflation. |
| **4. Which vehicles are highest/lowest risk?** | **Highest**: *CRAFTER* (Avg Claim: R76.7k)<br>**Lowest**: *TAZZ*, *SPRINTER DC* (Avg Claim: R7.3kâ€“R11.3k) | Target low-risk models with **5â€“10% premium discounts** to gain market share in private/commercial segments. |

### Sample Visualizations  
*(See full plots in `notebooks/eda_stats.ipynb`)*  
- **Loss Ratio by Province**: Eastern Cape (LR=1.3) vs Western Cape (LR=0.92)  
- **Log-Scaled Claim Distribution**: Exposes heavy tail & justifies transformation  
- **Top/Bottom 10 Vehicle Models by Avg Claim**: Clear segmentation opportunity  

---

## Task 2: Data Version Control (DVC)

### Why DVC?
In regulated industries like insurance, **auditability and reproducibility** are non-negotiable. DVC ensures:
-  Raw data is **versioned separately** from code (no Git bloat)
-  Anyone can `dvc pull` and **exactly reproduce** your analysis
-  Compliance-ready for regulators (e.g., FSCA in South Africa)

### Setup Steps
```bash
# 1. Initialize DVC
dvc init

# 2. Configure local remote (outside repo)
mkdir -p ../dvc-storage/
dvc remote add -d localstorage ../dvc-storage/

# 3. Track raw data
dvc add data/raw/MachineLearningRating_v3.txt

# 4. Commit metadata (NOT raw data!)
git add data/raw/*.dvc .dvc/config
git commit -m "feat(data): track raw dataset v1 with DVC"

# 5. Push to remote storage
dvc push
```

###  Reproduce This Analysis
```bash
git clone https://github.com/muhajirhualis/KAIM_Week3
cd KAIM_Week3
pip install -r requirements.txt dvc
dvc pull                          # Downloads raw data
jupyter notebook notebooks/eda_stats.ipynb
```

> ğŸ’¡ **Data Integrity**:  
> SHA256 of raw data (v1): `f6b7009b68ae21372b7deca9307fbb23`  
> *Exact hash auto-generated by DVC â€” check `data/raw/MachineLearningRating_v3.txt.dvc`*

---

## Next Steps (Tasks 3â€“4)
- **Task 3**: A/B hypothesis testing (e.g., *â€œIs risk significantly different by province?â€*)  
- **Task 4**: Predictive modeling for claim severity & risk-based premium optimization  
- **Final Report**: Medium-style blog with executive summary & data-backed recommendations

---

## ğŸ“š References & Tools
- [DVC Documentation](https://dvc.org/doc)
- [10 Academy Challenge Brief]
- Insurance Glossary: [50 Common Terms](https://cornerstonebrokers.co.za/insurance-terms)

---
  
